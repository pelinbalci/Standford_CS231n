{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TFsmoDqVeGOA"
   },
   "source": [
    "Ref: https://towardsdatascience.com/weight-initialization-in-neural-networks-a-journey-from-the-basics-to-kaiming-954fb9b47c79\n",
    "\n",
    "James Dellinger's paper is really explanable to understand the initialization topic. I thank to him for this great paper. I copy and paste all text and codes from this paper. You can check the website for more information. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "cB5uJDWLZOxO"
   },
   "outputs": [],
   "source": [
    "from torch.nn import init\n",
    "import torch\n",
    "import math\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "3AxkHTp_ZSoB",
    "outputId": "f730c353-f148-4f70-e779-bdd3e4c1eefc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x mean:  tensor(-0.0782) x std:  tensor(0.9463)\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(512)\n",
    "print('x mean: ', x.mean(), 'x std: ', x.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ufDCsDJkZYNI"
   },
   "source": [
    "Let’s also pretend that we have a simple 100-layer network with no activations ,\n",
    "and that each layer has a matrix a that contains the layer’s weights.\n",
    "\n",
    "It turns out that initializing the values of layer weights from the same standard NORMAL distribution to which we scaled our inputs is NEVER a good idea."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "-q9zRL-PZcDa",
    "outputId": "c32c9525-f2ce-40ef-8f58-5bad1c2cfe15"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x mean:  tensor(nan) x std:  tensor(nan)\n"
     ]
    }
   ],
   "source": [
    "for i in range(100):\n",
    "    a = torch.randn(512, 512)\n",
    "    x = torch.matmul(a, x)\n",
    "\n",
    "print('x mean: ', x.mean(), 'x std: ', x.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UYciKPqRZkG4"
   },
   "source": [
    "Why are they nan?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "NI5M4UGQZnLO",
    "outputId": "9fe28978-4783-481b-86a6-da5a5740b3f6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(512)\n",
    "for i in range(100):\n",
    "    a = torch.randn(512, 512)\n",
    "    x = torch.matmul(a, x)\n",
    "    if torch.isnan(x.std()): break\n",
    "\n",
    "print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qtai8z8SZrPd"
   },
   "source": [
    "The activation outputs exploded within 27 of our network’s layers.\n",
    "\n",
    "We clearly initialized our weights to be too large.\n",
    "\n",
    "Let's use small weights for initialization by multiply 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "Yh0eLp7CZp7E",
    "outputId": "ff2190a4-6064-46dc-e008-cea0a290156f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x mean:  tensor(0.) x std:  tensor(0.)\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(512)\n",
    "for i in range(100):\n",
    "    a = torch.randn(512, 512) * 0.01\n",
    "    x = torch.matmul(a, x)\n",
    "\n",
    "print('x mean: ', x.mean(), 'x std: ', x.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ru1kgbX2Alai"
   },
   "source": [
    "They are vanished :("
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JtdKQde1ZyAq"
   },
   "source": [
    "The matrix product of our inputs x and weight matrix a that we initialized from a standard normal distribution will, \n",
    "on average, have a standard deviation very close to the square root of the number of input connections, \n",
    "which in our example is sqrt(512)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "id": "aCkHUP2AZzmQ",
    "outputId": "7c42c4e1-65e7-421c-b156-9fc903fa8d41"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean:  0.0016365561494603752 std:  22.617465906573933\n",
      "sqrt of number of inputs:  22.627416997969522\n"
     ]
    }
   ],
   "source": [
    "mean, var = 0.0, 0.0\n",
    "y_sum, y_var = 0.0, 0.0\n",
    "for i in range(10000):\n",
    "    x = torch.randn(512)\n",
    "    a = torch.randn(512, 512)\n",
    "    y = torch.matmul(a, x)\n",
    "    mean += y.mean().item()\n",
    "    var += y.pow(2).mean().item()\n",
    "    y_sum += y.sum().item()\n",
    "\n",
    "print('mean: ', mean/10000, 'std: ', math.sqrt(var/10000))\n",
    "\n",
    "print('sqrt of number of inputs: ', math.sqrt(512))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "QKHu2GqsaIci",
    "outputId": "bcc2715c-b27c-49eb-9ab0-7cc0ade83b2b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0016365561494603752"
      ]
     },
     "execution_count": 8,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_sum/10000/512 # for 512 inputs we do the matrix mult. for 10000 times. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "id": "mOlAzlvQaALN",
    "outputId": "b51f9d3b-7951-44d0-d15e-fd961271895f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean for 1 input:  0.010211637498661753 std for 1 input:  0.9880182666932059\n"
     ]
    }
   ],
   "source": [
    "# The same for 1 input:\n",
    "mean_1, var_1, y_sum_1 = 0.0, 0.0, 0.0\n",
    "for i in range(10000):\n",
    "    x = torch.randn(1)\n",
    "    a = torch.randn(1)\n",
    "    y = torch.matmul(a, x)\n",
    "    mean_1 += y.mean().item()\n",
    "    var_1 += y.pow(2).mean().item()\n",
    "    y_sum_1 += y.sum().item()\n",
    "\n",
    "\n",
    "print('mean for 1 input: ', mean_1/10000, 'std for 1 input: ', math.sqrt(var_1/10000))\n",
    "# mean for 1 input:  -0.00942440634764862 std for 1 input:  1.0046721221399195"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GUE4NsobBrJ7"
   },
   "source": [
    "It then follows that the sum of these 512 products would have a mean of 0,  variance of 512, and therefore a standard deviation of √512.\n",
    "\n",
    "What we’d like is for each layer’s outputs to have a standard deviation of about 1. \n",
    "\n",
    "If we first scale the weight matrix a by dividing all its randomly chosen values by √512, the element-wise multiplication that fills in one element of the outputs y would now, on average, have a variance of only 1/√512\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "id": "8ufAdWooBtxn",
    "outputId": "84076e0e-4a31-470f-b4bc-765d2aa0f3ee"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean:  -0.0003472797142562627 var:  0.0018953190911959602\n",
      "check the var  0.001953125\n"
     ]
    }
   ],
   "source": [
    "mean, var = 0.0, 0.0\n",
    "for i in range(10000):\n",
    "    x = torch.randn(1)\n",
    "    a = torch.randn(1) * math.sqrt(1./512)\n",
    "    y = torch.matmul(a, x)\n",
    "    mean += y.item()\n",
    "    var += y.pow(2).item()\n",
    "\n",
    "print('mean: ', mean/10000, 'var: ', var/10000)\n",
    "\n",
    "print('check the var ', 1/512)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AlTRRObTDC_Z"
   },
   "source": [
    "Using 1/input --> achieve 1 std:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "9Ole87KrDB-M",
    "outputId": "bdbf991b-0391-4b11-a83a-cf4f716faa59"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean:  -0.00012715190263697877 std:  0.999678050607976\n"
     ]
    }
   ],
   "source": [
    "mean, var = 0.0, 0.0\n",
    "y_sum, y_var = 0.0, 0.0\n",
    "for i in range(10000):\n",
    "    x = torch.randn(512)\n",
    "    a = torch.randn(512, 512) * math.sqrt(1/512)\n",
    "    y = torch.matmul(a, x)\n",
    "    mean += y.mean().item()\n",
    "    var += y.pow(2).mean().item()\n",
    "    y_sum += y.sum().item()\n",
    "\n",
    "print('mean: ', mean/10000, 'std: ', math.sqrt(var/10000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "-DXh0ZJsDVSE",
    "outputId": "903c441c-7920-4f6e-af63-b399eee5c9c5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean:  tensor(-0.1187) std:  tensor(1.1257)\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(512)\n",
    "\n",
    "for i in range(100):  # 100 layer network\n",
    "    a = torch.randn(512, 512) * math.sqrt(1/512)\n",
    "    x = torch.matmul(a, x)\n",
    "\n",
    "print('mean: ', x.mean(), 'std: ', x.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sEspXqIUD2aW"
   },
   "source": [
    "Success! Our layer outputs neither exploded nor vanished, even after 100 of our hypothetical layers.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5wbNK4lIEMVn"
   },
   "source": [
    "## Xavier Initialization\n",
    "\n",
    "Let's use activation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "8bsl07WMEPo4"
   },
   "outputs": [],
   "source": [
    "def tanh(x):\n",
    "  return torch.tanh(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "AiujdLlVEcZF",
    "outputId": "65c9d2ba-8748-4a33-d9b1-8c23c5c92bf3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean:  tensor(-0.0036) std:  tensor(0.0933)\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(512)\n",
    "\n",
    "for i in range(100):  # 100 layer network\n",
    "    a = torch.randn(512, 512) * math.sqrt(1/512)\n",
    "    x = tanh(torch.matmul(a, x))\n",
    "\n",
    "print('mean: ', x.mean(), 'std: ', x.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VbaZG6GsEq4j"
   },
   "source": [
    "The standard deviation of activation outputs of the 100th layer is down to about 0.06. This is definitely on the small side, but at least activations haven’t totally vanished!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Urmg4PUyHiXN"
   },
   "source": [
    "When Xavier Glorot and Yoshua Bengio published their landmark paper titled Understanding the difficulty of training deep feedforward neural networks, the “commonly used heuristic” to which they compared their experiments was that of initializing weights from a uniform distribution in [-1,1] and then scaling by 1/√n."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "gdjNDahBHktj",
    "outputId": "e4ade62e-900a-407d-dda3-acc270d11524"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean:  tensor(-5.3748e-26) std:  tensor(8.7262e-25)\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(512)\n",
    "\n",
    "for i in range(100):  # 100 layer network\n",
    "    a = torch.Tensor(512, 512).uniform_(-1,1) * math.sqrt(1/512)\n",
    "    x = tanh(torch.matmul(a, x))\n",
    "\n",
    "print('mean: ', x.mean(), 'std: ', x.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QkPESMnKHu4R"
   },
   "source": [
    " They’re just about as good as vanished with uniform distribution :("
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lVlhAGRiH2NG"
   },
   "source": [
    "Xavier initialization sets a layer’s weights to values chosen from a random uniform distribution that’s bounded between : +- √6 / (√ni + ni+1)\n",
    "\n",
    "- nᵢ: the number of incoming network connections, or “fan-in,” to the layer. \n",
    "\n",
    "- nᵢ₊₁: the number of outgoing network connections from that layer, also known as the “fan-out.”"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W9UoK76aIQPc"
   },
   "source": [
    "Let's use Xavier Initialization for our 100 layer network with input 512:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "FxOqoVTTIVoL"
   },
   "outputs": [],
   "source": [
    "def xavier(m,h):\n",
    "  return torch.Tensor(m,h).uniform_(-1,1) * math.sqrt(6./(m+h))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "apNjMmQAIimI",
    "outputId": "76b344c7-7ba2-47cb-be0a-1370e1b8ae91"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean:  tensor(0.0047) std:  tensor(0.0729)\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(512)\n",
    "m = 512\n",
    "h = 512\n",
    "\n",
    "for i in range(100):  # 100 layer network\n",
    "    a = xavier(m,h)\n",
    "    x = tanh(torch.matmul(a, x))\n",
    "\n",
    "print('mean: ', x.mean(), 'std: ', x.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wcnsU7PQIyjy"
   },
   "source": [
    "In our experimental network, Xavier initialization performs pretty identical to the home-grown method that we derived earlier, where we sampled values from a random normal distribution and scaled by the square root of number of incoming network connections, n."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "exuwAQnOI1Wu"
   },
   "source": [
    "## Kaiming Initialization\n",
    "\n",
    "\n",
    "When using activation functions that are symmetric about zero and have outputs inside [-1,1], such as softsign and tanh:\n",
    "\n",
    "we’d want the activation outputs of each layer to have a MEAN of 0 and a STD around 1, on average. \n",
    "\n",
    "\n",
    "This is precisely what our home-grown method and Xavier both enable.\n",
    "\n",
    "But what if we’re using ReLU activation functions? Would it still make sense to want to scale random initial weight values in the same way?\n",
    "\n",
    "Let's use ReLu:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "a_CziLhVJOBc"
   },
   "outputs": [],
   "source": [
    "def relu(x):\n",
    "  return x.clamp_min(0.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "twxE_8yRJSko",
    "outputId": "2c0d5d4c-1f1c-4958-c215-9530d8776db8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean:  9.00831508398056 std:  15.981985173101688\n"
     ]
    }
   ],
   "source": [
    "mean, var= 0., 0.\n",
    "y_sum, y_var = 0.0, 0.0\n",
    "\n",
    "for i in range(10000):\n",
    "\n",
    "    x = torch.randn(512)\n",
    "    a = torch.randn(512, 512)\n",
    "    y = relu(torch.matmul(a, x))\n",
    "\n",
    "    mean += y.mean().item()\n",
    "    var += y.pow(2).mean().item()\n",
    "    y_sum += y.sum().item()\n",
    "\n",
    "print('mean: ', mean/10000, 'std: ', math.sqrt(var/10000))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "1z3XQiRvJkEp",
    "outputId": "3aedbe6d-5cd5-41bd-ef2c-3071d819da56"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16.0"
      ]
     },
     "execution_count": 28,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "math.sqrt(512/2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h8KAT8E-JpmN"
   },
   "source": [
    "when using a ReLU activation, a single layer will, on average have standard deviation that’s very close to the square root of the number of input connections, divided by the square root of two, or √512/√2 in our example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "kp24Uyd4JtVq",
    "outputId": "7f53962e-b52c-4846-b20e-cacfb21801e9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean:  0.5641567468941212 std:  1.0005376670522719\n"
     ]
    }
   ],
   "source": [
    "mean, var= 0., 0.\n",
    "y_sum, y_var = 0.0, 0.0\n",
    "\n",
    "for i in range(10000):\n",
    "\n",
    "    x = torch.randn(512)\n",
    "    a = torch.randn(512, 512) * math.sqrt(2/512)\n",
    "    y = relu(torch.matmul(a, x))\n",
    "\n",
    "    mean += y.mean().item()\n",
    "    var += y.pow(2).mean().item()\n",
    "    y_sum += y.sum().item()\n",
    "\n",
    "print('mean: ', mean/10000, 'std: ', math.sqrt(var/10000))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZceEOxUrJ1ur"
   },
   "source": [
    "Std is now close to 1. \n",
    "\n",
    "Why this is so important?\n",
    "\n",
    "Keeping the standard deviation of layers’ activations around 1 will allow us to stack several more layers in a deep neural network without gradients exploding or vanishing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VmnPXk9vKBgE"
   },
   "source": [
    "In their 2015 paper, He et. al. demonstrated that deep networks (e.g. a 22-layer CNN) would converge much earlier if the following input weight initialization strategy is employed:\n",
    "\n",
    "- Create a tensor with the dimensions appropriate for a weight matrix at a given layer, and populate it with numbers randomly chosen from a standard normal distribution.\n",
    "\n",
    "- Multiply each randomly chosen number by √2/√n where n is the number of incoming connections coming into a given layer from the previous layer’s output (also known as the “fan-in”).\n",
    "\n",
    "- Bias tensors are initialized to zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "4cPxCsoJKoEa"
   },
   "outputs": [],
   "source": [
    "def kaiming(m,h):\n",
    "  return torch.randn(m,h) * math.sqrt(2. / m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "Z37_4yObKvRC",
    "outputId": "24f463b4-17c6-4b29-b88a-d82668c7fbb4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean:  tensor(0.1514) std:  tensor(0.2260)\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(512)\n",
    "m = 512\n",
    "h = 512\n",
    "\n",
    "for i in range(100):  # 100 layer network\n",
    "    a = kaiming(m,h)\n",
    "    x = relu(torch.matmul(a, x))\n",
    "\n",
    "print('mean: ', x.mean(), 'std: ', x.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q-3KZ446K93Z"
   },
   "source": [
    "What if we use xavier instead?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "L_4vodc0LAIw",
    "outputId": "45daf8e2-24e6-44b9-cb62-fc312413b17d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean:  tensor(4.8009e-16) std:  tensor(6.4892e-16)\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(512)\n",
    "m = 512\n",
    "h = 512\n",
    "\n",
    "for i in range(100):  # 100 layer network\n",
    "    a = xavier(m,h)\n",
    "    x = relu(torch.matmul(a, x))\n",
    "\n",
    "print('mean: ', x.mean(), 'std: ', x.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4FEtIBu4LC6G"
   },
   "source": [
    "Vanished!"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "initialization.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
